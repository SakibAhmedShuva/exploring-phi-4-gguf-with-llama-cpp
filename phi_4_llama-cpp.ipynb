{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGJGbKGDWV4G",
        "outputId": "81bed989-5eeb-4fe1-e5f4-97aaddb585e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyXT12lB8ZDG",
        "outputId": "994aea6c-42c5-484d-ced6-770d6c09abbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 33 key-value pairs and 243 tensors from /root/.cache/huggingface/hub/models--microsoft--phi-4-gguf/snapshots/b1e764cfdbdd0a3ed824d6a8424129eb0a2232ff/./phi-4-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 4\n",
            "llama_model_loader: - kv   3:                            general.version str              = 4\n",
            "llama_model_loader: - kv   4:                       general.organization str              = Microsoft\n",
            "llama_model_loader: - kv   5:                           general.basename str              = phi\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   7:                            general.license str              = mit\n",
            "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/phi-...\n",
            "llama_model_loader: - kv   9:                               general.tags arr[str,7]       = [\"phi\", \"nlp\", \"math\", \"code\", \"chat\"...\n",
            "llama_model_loader: - kv  10:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  11:                        phi3.context_length u32              = 16384\n",
            "llama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 16384\n",
            "llama_model_loader: - kv  13:                      phi3.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 17920\n",
            "llama_model_loader: - kv  15:                           phi3.block_count u32              = 40\n",
            "llama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 10\n",
            "llama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 250000.000000\n",
            "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 0\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = dbrx\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 100257\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q4_K:  101 tensors\n",
            "llama_model_loader: - type q5_K:   40 tensors\n",
            "llama_model_loader: - type q6_K:   21 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 8.43 GiB (4.94 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 100350 '<|dummy_86|>' is not marked as EOG\n",
            "load: control token: 100349 '<|dummy_85|>' is not marked as EOG\n",
            "load: control token: 100348 '<|dummy_84|>' is not marked as EOG\n",
            "load: control token: 100347 '<|dummy_83|>' is not marked as EOG\n",
            "load: control token: 100346 '<|dummy_82|>' is not marked as EOG\n",
            "load: control token: 100343 '<|dummy_79|>' is not marked as EOG\n",
            "load: control token: 100341 '<|dummy_77|>' is not marked as EOG\n",
            "load: control token: 100339 '<|dummy_75|>' is not marked as EOG\n",
            "load: control token: 100338 '<|dummy_74|>' is not marked as EOG\n",
            "load: control token: 100335 '<|dummy_71|>' is not marked as EOG\n",
            "load: control token: 100334 '<|dummy_70|>' is not marked as EOG\n",
            "load: control token: 100333 '<|dummy_69|>' is not marked as EOG\n",
            "load: control token: 100330 '<|dummy_66|>' is not marked as EOG\n",
            "load: control token: 100328 '<|dummy_64|>' is not marked as EOG\n",
            "load: control token: 100327 '<|dummy_63|>' is not marked as EOG\n",
            "load: control token: 100326 '<|dummy_62|>' is not marked as EOG\n",
            "load: control token: 100325 '<|dummy_61|>' is not marked as EOG\n",
            "load: control token: 100324 '<|dummy_60|>' is not marked as EOG\n",
            "load: control token: 100323 '<|dummy_59|>' is not marked as EOG\n",
            "load: control token: 100322 '<|dummy_58|>' is not marked as EOG\n",
            "load: control token: 100321 '<|dummy_57|>' is not marked as EOG\n",
            "load: control token: 100319 '<|dummy_55|>' is not marked as EOG\n",
            "load: control token: 100318 '<|dummy_54|>' is not marked as EOG\n",
            "load: control token: 100317 '<|dummy_53|>' is not marked as EOG\n",
            "load: control token: 100316 '<|dummy_52|>' is not marked as EOG\n",
            "load: control token: 100314 '<|dummy_50|>' is not marked as EOG\n",
            "load: control token: 100313 '<|dummy_49|>' is not marked as EOG\n",
            "load: control token: 100312 '<|dummy_48|>' is not marked as EOG\n",
            "load: control token: 100309 '<|dummy_45|>' is not marked as EOG\n",
            "load: control token: 100308 '<|dummy_44|>' is not marked as EOG\n",
            "load: control token: 100304 '<|dummy_40|>' is not marked as EOG\n",
            "load: control token: 100303 '<|dummy_39|>' is not marked as EOG\n",
            "load: control token: 100301 '<|dummy_37|>' is not marked as EOG\n",
            "load: control token: 100299 '<|dummy_35|>' is not marked as EOG\n",
            "load: control token: 100298 '<|dummy_34|>' is not marked as EOG\n",
            "load: control token: 100297 '<|dummy_33|>' is not marked as EOG\n",
            "load: control token: 100296 '<|dummy_32|>' is not marked as EOG\n",
            "load: control token: 100295 '<|dummy_31|>' is not marked as EOG\n",
            "load: control token: 100293 '<|dummy_29|>' is not marked as EOG\n",
            "load: control token: 100291 '<|dummy_27|>' is not marked as EOG\n",
            "load: control token: 100289 '<|dummy_25|>' is not marked as EOG\n",
            "load: control token: 100288 '<|dummy_24|>' is not marked as EOG\n",
            "load: control token: 100287 '<|dummy_23|>' is not marked as EOG\n",
            "load: control token: 100286 '<|dummy_22|>' is not marked as EOG\n",
            "load: control token: 100285 '<|dummy_21|>' is not marked as EOG\n",
            "load: control token: 100279 '<|dummy_15|>' is not marked as EOG\n",
            "load: control token: 100276 '<|endofprompt|>' is not marked as EOG\n",
            "load: control token: 100274 '<|dummy_11|>' is not marked as EOG\n",
            "load: control token: 100273 '<|dummy_10|>' is not marked as EOG\n",
            "load: control token: 100272 '<|dummy_9|>' is not marked as EOG\n",
            "load: control token: 100271 '<|dummy_8|>' is not marked as EOG\n",
            "load: control token: 100269 '<|dummy_6|>' is not marked as EOG\n",
            "load: control token: 100266 '<|im_sep|>' is not marked as EOG\n",
            "load: control token: 100261 '<|dummy_1|>' is not marked as EOG\n",
            "load: control token: 100259 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 100258 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 100256 '<|dummy_0|>' is not marked as EOG\n",
            "load: control token: 100331 '<|dummy_67|>' is not marked as EOG\n",
            "load: control token: 100263 '<|dummy_3|>' is not marked as EOG\n",
            "load: control token: 100307 '<|dummy_43|>' is not marked as EOG\n",
            "load: control token: 100270 '<|dummy_7|>' is not marked as EOG\n",
            "load: control token: 100305 '<|dummy_41|>' is not marked as EOG\n",
            "load: control token: 100282 '<|dummy_18|>' is not marked as EOG\n",
            "load: control token: 100345 '<|dummy_81|>' is not marked as EOG\n",
            "load: control token: 100351 '<|dummy_87|>' is not marked as EOG\n",
            "load: control token: 100262 '<|dummy_2|>' is not marked as EOG\n",
            "load: control token: 100292 '<|dummy_28|>' is not marked as EOG\n",
            "load: control token: 100294 '<|dummy_30|>' is not marked as EOG\n",
            "load: control token: 100302 '<|dummy_38|>' is not marked as EOG\n",
            "load: control token: 100278 '<|dummy_14|>' is not marked as EOG\n",
            "load: control token: 100310 '<|dummy_46|>' is not marked as EOG\n",
            "load: control token: 100275 '<|dummy_12|>' is not marked as EOG\n",
            "load: control token: 100290 '<|dummy_26|>' is not marked as EOG\n",
            "load: control token: 100340 '<|dummy_76|>' is not marked as EOG\n",
            "load: control token: 100277 '<|dummy_13|>' is not marked as EOG\n",
            "load: control token: 100320 '<|dummy_56|>' is not marked as EOG\n",
            "load: control token: 100337 '<|dummy_73|>' is not marked as EOG\n",
            "load: control token: 100283 '<|dummy_19|>' is not marked as EOG\n",
            "load: control token: 100329 '<|dummy_65|>' is not marked as EOG\n",
            "load: control token: 100284 '<|dummy_20|>' is not marked as EOG\n",
            "load: control token: 100267 '<|dummy_4|>' is not marked as EOG\n",
            "load: control token: 100344 '<|dummy_80|>' is not marked as EOG\n",
            "load: control token: 100332 '<|dummy_68|>' is not marked as EOG\n",
            "load: control token: 100306 '<|dummy_42|>' is not marked as EOG\n",
            "load: control token: 100268 '<|dummy_5|>' is not marked as EOG\n",
            "load: control token: 100264 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 100281 '<|dummy_17|>' is not marked as EOG\n",
            "load: control token: 100342 '<|dummy_78|>' is not marked as EOG\n",
            "load: control token: 100311 '<|dummy_47|>' is not marked as EOG\n",
            "load: control token: 100280 '<|dummy_16|>' is not marked as EOG\n",
            "load: control token: 100315 '<|dummy_51|>' is not marked as EOG\n",
            "load: control token: 100336 '<|dummy_72|>' is not marked as EOG\n",
            "load: control token: 100260 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 100300 '<|dummy_36|>' is not marked as EOG\n",
            "load: special tokens cache size = 96\n",
            "load: token to piece cache size = 0.6151 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 16384\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 10\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1280\n",
            "print_info: n_embd_v_gqa     = 1280\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 17920\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 250000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 16384\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.66 B\n",
            "print_info: general.name     = Phi 4\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 100352\n",
            "print_info: n_merges         = 100000\n",
            "print_info: BOS token        = 100257 '<|endoftext|>'\n",
            "print_info: EOS token        = 100257 '<|endoftext|>'\n",
            "print_info: EOT token        = 100265 '<|im_end|>'\n",
            "print_info: PAD token        = 100257 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
            "print_info: EOG token        = 100257 '<|endoftext|>'\n",
            "print_info: EOG token        = 100265 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: layer  37 assigned to device CPU\n",
            "load_tensors: layer  38 assigned to device CPU\n",
            "load_tensors: layer  39 assigned to device CPU\n",
            "load_tensors: layer  40 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 242 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  8630.33 MiB\n",
            ".........................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 16384\n",
            "llama_init_from_model: n_ctx_per_seq = 16384\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 250000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
            "llama_init_from_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.38 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =  1357.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1606\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '100257', 'tokenizer.ggml.bos_token_id': '100257', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'phi3.rope.freq_base': '250000.000000', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '10', 'phi3.attention.head_count': '40', 'phi3.block_count': '40', 'tokenizer.ggml.padding_token_id': '100257', 'general.basename': 'phi', 'tokenizer.ggml.pre': 'dbrx', 'general.name': 'Phi 4', 'phi3.rope.dimension_count': '128', 'general.version': '4', 'phi3.attention.sliding_window': '0', 'general.organization': 'Microsoft', 'general.type': 'model', 'general.size_label': '15B', 'tokenizer.chat_template': \"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|><|im_start|>assistant<|im_sep|>'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}\", 'general.license.link': 'https://huggingface.co/microsoft/phi-4/resolve/main/LICENSE', 'general.license': 'mit', 'general.architecture': 'phi3', 'phi3.context_length': '16384', 'phi3.feed_forward_length': '17920', 'phi3.rope.scaling.original_context_length': '16384', 'phi3.embedding_length': '5120'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|><|im_start|>assistant<|im_sep|>'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"microsoft/phi-4-gguf\",\n",
        "    filename=\"phi-4-q4.gguf\",\n",
        "    n_gpu_layers=-1,  # Offload all layers to GPU\n",
        "    n_ctx=16384,        # Context size is set to 512 based on the logs\n",
        "    verbose=True      # Optional: to see detailed loading info\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lmIDyFIt_R1i"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a JSON-only response system. Follow these rules absolutely:\n",
        "1. ONLY output valid, parseable JSON\n",
        "2. NEVER include text before or after the JSON\n",
        "3. NEVER include markdown code blocks or formatting\n",
        "4. NEVER include explanations\n",
        "5. If you can't fulfill a request, return {\"error\": \"error message\"}\n",
        "6. Output should always be a single JSON object\n",
        "\n",
        "For address requests, use this format:\n",
        "{\n",
        "    \"address\": {\n",
        "        \"license\": \"B1231241\",\n",
        "        \"Address\": \"X City\",\n",
        "        \"Sex\": \"Male\",\n",
        "        \"Weight\": \"X\",\n",
        "        \"Height\": \"X\"\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e_82OPlp-UuZ"
      },
      "outputs": [],
      "source": [
        "test_instruction = '''extract NER:\n",
        "        California\n",
        "        DRIVER LICENSe\n",
        "        dl 11234568\n",
        "        CLASS C\n",
        "        EXP 08/31/2014\n",
        "        END NONE\n",
        "        LNCARDHOLDER FNIMA\n",
        "        2570 24TH STREET ANYTOWN, CA 95818\n",
        "        doB 08/31/1977 RSTR NONE\n",
        "        08311977\n",
        "        VETERAN\n",
        "        Cordhslde\n",
        "        SEX F HGT 5'-05\"\n",
        "        HAIR BRN WGT 125 lb\n",
        "        EYES BRN\n",
        "        DD 00/00/0000NNNAN/ANFD/YY\n",
        "        ISS 08/31/2009\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCgnw-rE_D4e",
        "outputId": "e9de1ec0-5e77-47f7-db96-ffa7780b1058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting text generation...\n",
            "```json\n",
            "{\n",
            "    \"address\": {\n",
            "        \"license\": \"dl 11234568\",\n",
            "        \"Address\": \"2570 24th Street, Anytown, CA 95818\",\n",
            "        \"Sex\": \"F\",\n",
            "        \"Weight\": \"125 lb\",\n",
            "        \"Height\": \"5'-05\\\"\"\n",
            "    },\n",
            "    \"additional_info\": {\n",
            "        \"state\": \"California\",\n",
            "        \"license_class\": \"CLASS C\",\n",
            "        \"expiration_date\": \"08/31/2014\",\n",
            "        \"license_holder_name\": \"LNCARDHOLDER FNIMA\",\n",
            "        \"date_of_birth\": \"08/31/1977\",\n",
            "        \"veteran_status\": \"VETERAN\",\n",
            "        \"hair_color\": \"BRN\",\n",
            "        \"eye_color\": \"BRN\"\n",
            "    }\n",
            "}\n",
            "```"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  205192.00 ms\n",
            "llama_perf_context_print: prompt eval time =  205191.39 ms /   296 tokens (  693.21 ms per token,     1.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =  204388.84 ms /   164 runs   ( 1246.27 ms per token,     0.80 tokens per second)\n",
            "llama_perf_context_print:       total time =  409932.77 ms /   460 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Time taken: 409.9410 seconds\n"
          ]
        }
      ],
      "source": [
        "import time  # Import the time module\n",
        "\n",
        "# --- Assuming generate_text, model, tokenizer, test_instruction are defined elsewhere ---\n",
        "\n",
        "print(\"Starting text generation...\") # Optional: Indicate start\n",
        "\n",
        "start_time = time.perf_counter()  # Get the time just before the call\n",
        "\n",
        "####################\n",
        "\n",
        "\n",
        "\n",
        "for chunk in llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": test_instruction\n",
        "        }\n",
        "    ],\n",
        "    stream=True\n",
        "):\n",
        "    print(chunk['choices'][0]['delta'].get('content', ''), end='', flush=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "end_time = time.perf_counter()    # Get the time just after the call\n",
        "\n",
        "elapsed_time = end_time - start_time # Calculate the difference\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"\\nTime taken: {elapsed_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiYx9kVFD5kq"
      },
      "source": [
        "# Phi-4-mini-instruct-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBZKtXqWsKED",
        "outputId": "d45c0bc8-270a-4142-859a-d7a945a42e51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 35 key-value pairs and 196 tensors from /root/.cache/huggingface/hub/models--unsloth--Phi-4-mini-instruct-GGUF/snapshots/78eb92a46fc37e6b524df991ed9aca9bc6aa7b80/./Phi-4-mini-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv   2:                               general.type str              = model\n",
            "llama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\n",
            "llama_model_loader: - kv   4:                       general.organization str              = Microsoft\n",
            "llama_model_loader: - kv   5:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   6:                           general.basename str              = Phi-4\n",
            "llama_model_loader: - kv   7:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   8:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   9:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  20:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = gpt-4o\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 199999\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 200020\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3251\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 200029\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q4_K:   80 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.31 GiB (5.18 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 200024 '<|/tool|>' is not marked as EOG\n",
            "load: control token: 200023 '<|tool|>' is not marked as EOG\n",
            "load: control token: 200022 '<|system|>' is not marked as EOG\n",
            "load: control token: 200021 '<|user|>' is not marked as EOG\n",
            "load: control token: 200025 '<|tool_call|>' is not marked as EOG\n",
            "load: control token: 200027 '<|tool_response|>' is not marked as EOG\n",
            "load: control token:   3251 '�' is not marked as EOG\n",
            "load: control token: 200028 '<|tag|>' is not marked as EOG\n",
            "load: control token: 200026 '<|/tool_call|>' is not marked as EOG\n",
            "load: control token: 200029 '<｜PAD▁TOKEN｜>' is not marked as EOG\n",
            "load: control token: 200018 '<|endofprompt|>' is not marked as EOG\n",
            "load: control token: 200019 '<|assistant|>' is not marked as EOG\n",
            "load: special tokens cache size = 14\n",
            "load: token to piece cache size = 1.3333 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 24\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 262144\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 3\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.84 B\n",
            "print_info: general.name     = Phi 4 Mini Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 200064\n",
            "print_info: n_merges         = 199742\n",
            "print_info: BOS token        = 199999 '<|endoftext|>'\n",
            "print_info: EOS token        = 200020 '<|end|>'\n",
            "print_info: EOT token        = 199999 '<|endoftext|>'\n",
            "print_info: UNK token        = 3251 '�'\n",
            "print_info: PAD token        = 200029 '<｜PAD▁TOKEN｜>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 199999 '<|endoftext|>'\n",
            "print_info: EOG token        = 200020 '<|end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  2368.57 MiB\n",
            "..............................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.76 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   409.76 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'general.architecture': 'phi3', 'phi3.context_length': '131072', 'general.type': 'model', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.quantized_by': 'Unsloth', 'general.file_type': '15', 'general.finetune': 'instruct', 'general.repo_url': 'https://huggingface.co/unsloth', 'tokenizer.ggml.add_eos_token': 'false', 'general.organization': 'Microsoft', 'tokenizer.ggml.pre': 'gpt-4o', 'general.basename': 'Phi-4', 'tokenizer.ggml.padding_token_id': '200029', 'phi3.attention.head_count': '24', 'phi3.attention.head_count_kv': '8', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'phi3.attention.sliding_window': '262144', 'phi3.rope.freq_base': '10000.000000', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'gpt2', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 4 Mini Instruct', 'tokenizer.ggml.bos_token_id': '199999', 'tokenizer.ggml.unknown_token_id': '3251', 'tokenizer.ggml.eos_token_id': '200020', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}\n",
            "Using chat eos_token: <|end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"unsloth/Phi-4-mini-instruct-GGUF\",\n",
        "\tfilename=\"Phi-4-mini-instruct-Q4_K_M.gguf\",\n",
        " \tn_gpu_layers=-1  # Use -1 to offload all layers to GPU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKRo2rIxOG18",
        "outputId": "cf9ad863-9a7e-49c2-9e47-c735a602f55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting text generation...\n",
            "```json\n",
            "{\n",
            "    \"entities\": {\n",
            "        \"Location\": [\"California\"],\n",
            "        \"License\": [\"DRIVER LICENSe\"],\n",
            "        \"LicenseNumber\": [\"dl 11234568\"],\n",
            "        \"LicenseClass\": [\"CLASS C\"],\n",
            "        \"ExpirationDate\": [\"EXP 08/31/2014\"],\n",
            "        \"LicenseType\": [\"END NONE\"],\n",
            "        \"LicenseHolderName\": [\"LNCARDHOLDER FNIMA\"],\n",
            "        \"Address\": [\"2570 24TH STREET ANYTOWN, CA 95818\"],\n",
            "        \"BirthDate\": [\"doB 08/31/1977 RSTR NONE\"],\n",
            "        \"BirthYear\": [\"08311977\"],\n",
            "        \"MilitaryStatus\": [\"VETERAN\"],\n",
            "        \"Name\": [\"Cordhslde\"],\n",
            "        \"Gender\": [\"SEX F\"],\n",
            "        \"Height\": [\"HGT 5'-05\\\"\"],\n",
            "        \"HairColor\": [\"HAIR BRN\"],\n",
            "        \"HairWeight\": [\"WGT 125 lb\"],\n",
            "        \"EyeColor\": [\"EYES BRN\"],\n",
            "        \"DateOfBirth\": [\"DD 00/00/"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =   71097.75 ms\n",
            "llama_perf_context_print: prompt eval time =   71093.74 ms /   289 tokens (  246.00 ms per token,     4.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =  115361.40 ms /   222 runs   (  519.65 ms per token,     1.92 tokens per second)\n",
            "llama_perf_context_print:       total time =  187415.79 ms /   511 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Time taken: 187.4341 seconds\n"
          ]
        }
      ],
      "source": [
        "import time  # Import the time module\n",
        "\n",
        "# --- Assuming generate_text, model, tokenizer, test_instruction are defined elsewhere ---\n",
        "\n",
        "print(\"Starting text generation...\") # Optional: Indicate start\n",
        "\n",
        "start_time = time.perf_counter()  # Get the time just before the call\n",
        "\n",
        "####################\n",
        "\n",
        "\n",
        "\n",
        "for chunk in llm.create_chat_completion(\n",
        "\tmessages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": test_instruction\n",
        "        }\n",
        "\t],\n",
        "    stream=True\n",
        "):\n",
        "    print(chunk['choices'][0]['delta'].get('content', ''), end='', flush=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "end_time = time.perf_counter()    # Get the time just after the call\n",
        "\n",
        "elapsed_time = end_time - start_time # Calculate the difference\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"\\nTime taken: {elapsed_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNExgz7E4iTT"
      },
      "source": [
        "# phi-4-Q2_K.gguf (Slow for unknown reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne5uvXGG2wum",
        "outputId": "b31094dd-77ba-4da0-971a-f408591f8c30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 40 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--unsloth--phi-4-GGUF/snapshots/5110b7771e8166d5530e73346a15aea096a8cb99/./phi-4-Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 4\n",
            "llama_model_loader: - kv   3:                            general.version str              = 4\n",
            "llama_model_loader: - kv   4:                           general.basename str              = phi\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/phi-...\n",
            "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Phi 4\n",
            "llama_model_loader: - kv  10:               general.base_model.0.version str              = 4\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Microsoft\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/microsoft/phi-4\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,9]       = [\"phi\", \"phi4\", \"unsloth\", \"nlp\", \"ma...\n",
            "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  15:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv  16:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv  17:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 17920\n",
            "llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 10\n",
            "llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 250000.000000\n",
            "llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  25:                           llama.vocab_size u32              = 100352\n",
            "llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = dbrx\n",
            "llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 100265\n",
            "llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 5809\n",
            "llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 100351\n",
            "llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
            "llama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  38:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  39:                          general.file_type u32              = 10\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q2_K:  161 tensors\n",
            "llama_model_loader: - type q3_K:   80 tensors\n",
            "llama_model_loader: - type q4_K:   40 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 5.22 GiB (3.06 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 100350 '<|dummy_86|>' is not marked as EOG\n",
            "load: control token: 100349 '<|dummy_85|>' is not marked as EOG\n",
            "load: control token: 100348 '<|dummy_84|>' is not marked as EOG\n",
            "load: control token: 100347 '<|dummy_83|>' is not marked as EOG\n",
            "load: control token: 100346 '<|dummy_82|>' is not marked as EOG\n",
            "load: control token: 100343 '<|dummy_79|>' is not marked as EOG\n",
            "load: control token: 100341 '<|dummy_77|>' is not marked as EOG\n",
            "load: control token: 100339 '<|dummy_75|>' is not marked as EOG\n",
            "load: control token: 100338 '<|dummy_74|>' is not marked as EOG\n",
            "load: control token: 100335 '<|dummy_71|>' is not marked as EOG\n",
            "load: control token: 100334 '<|dummy_70|>' is not marked as EOG\n",
            "load: control token: 100333 '<|dummy_69|>' is not marked as EOG\n",
            "load: control token: 100330 '<|dummy_66|>' is not marked as EOG\n",
            "load: control token: 100328 '<|dummy_64|>' is not marked as EOG\n",
            "load: control token: 100327 '<|dummy_63|>' is not marked as EOG\n",
            "load: control token: 100326 '<|dummy_62|>' is not marked as EOG\n",
            "load: control token: 100325 '<|dummy_61|>' is not marked as EOG\n",
            "load: control token: 100324 '<|dummy_60|>' is not marked as EOG\n",
            "load: control token: 100323 '<|dummy_59|>' is not marked as EOG\n",
            "load: control token: 100322 '<|dummy_58|>' is not marked as EOG\n",
            "load: control token: 100321 '<|dummy_57|>' is not marked as EOG\n",
            "load: control token: 100319 '<|dummy_55|>' is not marked as EOG\n",
            "load: control token: 100318 '<|dummy_54|>' is not marked as EOG\n",
            "load: control token: 100317 '<|dummy_53|>' is not marked as EOG\n",
            "load: control token: 100316 '<|dummy_52|>' is not marked as EOG\n",
            "load: control token: 100314 '<|dummy_50|>' is not marked as EOG\n",
            "load: control token: 100313 '<|dummy_49|>' is not marked as EOG\n",
            "load: control token: 100312 '<|dummy_48|>' is not marked as EOG\n",
            "load: control token: 100309 '<|dummy_45|>' is not marked as EOG\n",
            "load: control token: 100308 '<|dummy_44|>' is not marked as EOG\n",
            "load: control token: 100304 '<|dummy_40|>' is not marked as EOG\n",
            "load: control token: 100303 '<|dummy_39|>' is not marked as EOG\n",
            "load: control token: 100301 '<|dummy_37|>' is not marked as EOG\n",
            "load: control token: 100299 '<|dummy_35|>' is not marked as EOG\n",
            "load: control token: 100298 '<|dummy_34|>' is not marked as EOG\n",
            "load: control token: 100297 '<|dummy_33|>' is not marked as EOG\n",
            "load: control token: 100296 '<|dummy_32|>' is not marked as EOG\n",
            "load: control token: 100295 '<|dummy_31|>' is not marked as EOG\n",
            "load: control token: 100293 '<|dummy_29|>' is not marked as EOG\n",
            "load: control token: 100291 '<|dummy_27|>' is not marked as EOG\n",
            "load: control token: 100289 '<|dummy_25|>' is not marked as EOG\n",
            "load: control token: 100288 '<|dummy_24|>' is not marked as EOG\n",
            "load: control token: 100287 '<|dummy_23|>' is not marked as EOG\n",
            "load: control token: 100286 '<|dummy_22|>' is not marked as EOG\n",
            "load: control token: 100285 '<|dummy_21|>' is not marked as EOG\n",
            "load: control token: 100279 '<|dummy_15|>' is not marked as EOG\n",
            "load: control token: 100276 '<|endofprompt|>' is not marked as EOG\n",
            "load: control token: 100274 '<|dummy_11|>' is not marked as EOG\n",
            "load: control token: 100273 '<|dummy_10|>' is not marked as EOG\n",
            "load: control token: 100272 '<|dummy_9|>' is not marked as EOG\n",
            "load: control token: 100271 '<|dummy_8|>' is not marked as EOG\n",
            "load: control token: 100269 '<|dummy_6|>' is not marked as EOG\n",
            "load: control token: 100266 '<|im_sep|>' is not marked as EOG\n",
            "load: control token: 100261 '<|dummy_1|>' is not marked as EOG\n",
            "load: control token: 100259 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 100258 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 100256 '<|dummy_0|>' is not marked as EOG\n",
            "load: control token: 100331 '<|dummy_67|>' is not marked as EOG\n",
            "load: control token: 100263 '<|dummy_3|>' is not marked as EOG\n",
            "load: control token: 100307 '<|dummy_43|>' is not marked as EOG\n",
            "load: control token: 100270 '<|dummy_7|>' is not marked as EOG\n",
            "load: control token: 100305 '<|dummy_41|>' is not marked as EOG\n",
            "load: control token: 100282 '<|dummy_18|>' is not marked as EOG\n",
            "load: control token: 100345 '<|dummy_81|>' is not marked as EOG\n",
            "load: control token: 100351 '<|dummy_87|>' is not marked as EOG\n",
            "load: control token: 100262 '<|dummy_2|>' is not marked as EOG\n",
            "load: control token: 100292 '<|dummy_28|>' is not marked as EOG\n",
            "load: control token: 100294 '<|dummy_30|>' is not marked as EOG\n",
            "load: control token: 100302 '<|dummy_38|>' is not marked as EOG\n",
            "load: control token: 100278 '<|dummy_14|>' is not marked as EOG\n",
            "load: control token: 100310 '<|dummy_46|>' is not marked as EOG\n",
            "load: control token:   5809 '�' is not marked as EOG\n",
            "load: control token: 100275 '<|dummy_12|>' is not marked as EOG\n",
            "load: control token: 100290 '<|dummy_26|>' is not marked as EOG\n",
            "load: control token: 100340 '<|dummy_76|>' is not marked as EOG\n",
            "load: control token: 100277 '<|dummy_13|>' is not marked as EOG\n",
            "load: control token: 100320 '<|dummy_56|>' is not marked as EOG\n",
            "load: control token: 100337 '<|dummy_73|>' is not marked as EOG\n",
            "load: control token: 100283 '<|dummy_19|>' is not marked as EOG\n",
            "load: control token: 100329 '<|dummy_65|>' is not marked as EOG\n",
            "load: control token: 100284 '<|dummy_20|>' is not marked as EOG\n",
            "load: control token: 100267 '<|dummy_4|>' is not marked as EOG\n",
            "load: control token: 100344 '<|dummy_80|>' is not marked as EOG\n",
            "load: control token: 100332 '<|dummy_68|>' is not marked as EOG\n",
            "load: control token: 100306 '<|dummy_42|>' is not marked as EOG\n",
            "load: control token: 100268 '<|dummy_5|>' is not marked as EOG\n",
            "load: control token: 100264 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 100281 '<|dummy_17|>' is not marked as EOG\n",
            "load: control token: 100342 '<|dummy_78|>' is not marked as EOG\n",
            "load: control token: 100311 '<|dummy_47|>' is not marked as EOG\n",
            "load: control token: 100280 '<|dummy_16|>' is not marked as EOG\n",
            "load: control token: 100315 '<|dummy_51|>' is not marked as EOG\n",
            "load: control token: 100336 '<|dummy_72|>' is not marked as EOG\n",
            "load: control token: 100260 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 100300 '<|dummy_36|>' is not marked as EOG\n",
            "load: special tokens cache size = 97\n",
            "load: token to piece cache size = 0.6151 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 16384\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 10\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1280\n",
            "print_info: n_embd_v_gqa     = 1280\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 17920\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 250000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 16384\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 13B\n",
            "print_info: model params     = 14.66 B\n",
            "print_info: general.name     = Phi 4\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 100352\n",
            "print_info: n_merges         = 100000\n",
            "print_info: BOS token        = 100257 '<|endoftext|>'\n",
            "print_info: EOS token        = 100265 '<|im_end|>'\n",
            "print_info: EOT token        = 100265 '<|im_end|>'\n",
            "print_info: UNK token        = 5809 '�'\n",
            "print_info: PAD token        = 100351 '<|dummy_87|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
            "print_info: EOG token        = 100257 '<|endoftext|>'\n",
            "print_info: EOG token        = 100265 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: layer  37 assigned to device CPU\n",
            "load_tensors: layer  38 assigned to device CPU\n",
            "load_tensors: layer  39 assigned to device CPU\n",
            "load_tensors: layer  40 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q2_K) (and 362 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  5345.57 MiB\n",
            "............................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 250000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (16384) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1280, n_embd_v_gqa = 1280\n",
            "llama_kv_cache_init:        CPU KV buffer size =   100.00 MiB\n",
            "llama_init_from_model: KV self size  =  100.00 MiB, K (f16):   50.00 MiB, V (f16):   50.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.38 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   206.00 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '10', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '100351', 'general.license': 'mit', 'llama.attention.value_length': '128', 'general.size_label': '15B', 'general.type': 'model', 'general.base_model.0.repo_url': 'https://huggingface.co/microsoft/phi-4', 'general.version': '4', 'general.base_model.0.name': 'Phi 4', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '128', 'general.base_model.0.version': '4', 'llama.attention.head_count_kv': '10', 'llama.context_length': '16384', 'llama.embedding_length': '5120', 'general.basename': 'phi', 'general.architecture': 'llama', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Microsoft', 'llama.feed_forward_length': '17920', 'general.name': 'Phi 4', 'tokenizer.ggml.bos_token_id': '100257', 'llama.rope.freq_base': '250000.000000', 'llama.block_count': '40', 'llama.attention.head_count': '40', 'llama.attention.key_length': '128', 'general.license.link': 'https://huggingface.co/microsoft/phi-4/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'dbrx', 'llama.vocab_size': '100352', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.unknown_token_id': '5809', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '100265'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm_Q2 = Llama.from_pretrained(\n",
        "\trepo_id=\"unsloth/phi-4-GGUF\",\n",
        "\tfilename=\"phi-4-Q2_K.gguf\",\n",
        " \tn_gpu_layers=-1  # Use -1 to offload all layers to GPU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSL4aRTG4WnN",
        "outputId": "f728c9d4-2e2a-45a3-b71a-678e1c405916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting text generation...\n",
            "```json\n",
            "{\n",
            "    \"address\": {\n",
            "        \"license\": \"dl 11234568\",\n",
            "        \"Address\": \"2570 24TH STREET ANYTOWN, CA 95818\",\n",
            "        \"Sex\": \"F\",\n",
            "        \"Weight\": \"125\",\n",
            "        \"Height\": \"5'-05\\\"\"\n",
            "    }\n",
            "}\n",
            "```"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  251872.46 ms\n",
            "llama_perf_context_print: prompt eval time =  251871.66 ms /   296 tokens (  850.92 ms per token,     1.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =   97491.32 ms /    68 runs   ( 1433.70 ms per token,     0.70 tokens per second)\n",
            "llama_perf_context_print:       total time =  349588.50 ms /   364 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Time taken: 349.6093 seconds\n"
          ]
        }
      ],
      "source": [
        "import time  # Import the time module\n",
        "\n",
        "# --- Assuming generate_text, model, tokenizer, test_instruction are defined elsewhere ---\n",
        "\n",
        "print(\"Starting text generation...\") # Optional: Indicate start\n",
        "\n",
        "start_time = time.perf_counter()  # Get the time just before the call\n",
        "\n",
        "####################\n",
        "\n",
        "\n",
        "\n",
        "for chunk in llm_Q2.create_chat_completion(\n",
        "\tmessages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": test_instruction\n",
        "        }\n",
        "\t],\n",
        "    stream=True\n",
        "):\n",
        "    print(chunk['choices'][0]['delta'].get('content', ''), end='', flush=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "end_time = time.perf_counter()    # Get the time just after the call\n",
        "\n",
        "elapsed_time = end_time - start_time # Calculate the difference\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"\\nTime taken: {elapsed_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTt5WzFaOt1c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
